{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "#define the parameters of the gaussian probability distribution\n",
    "mu = 0.0\n",
    "sigma = 1.0\n",
    "\n",
    "numDims = 1 #number of dimensions\n",
    "numSamples = 1000\n",
    "\n",
    "#Generate samples from gaussian normal distribution:\n",
    "X = np.random.normal(mu,sigma,[numSamples,numDims])\n",
    "\n",
    "empiricalMean = np.mean(X)\n",
    "empiricalStdDev = np.std(X)\n",
    "\n",
    "print(\"empirical=\",empiricalMean)\n",
    "print(\"empirical std=\",empiricalStdDev)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3035337a2778a39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "numBins = 30\n",
    "freq, bins, ignored = plt.hist(X,numBins, density=True )\n",
    "plt.bar(bins[:numBins], freq)\n",
    "plt.show()\n",
    "print(len(freq))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd8933eb3831e5fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_dataframe = pd.DataFrame(X) #Convert numpy to dataframe\n",
    "my_dataframe.to_csv(\"normalData1.csv\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c35244b3c9f737f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "numDims=2\n",
    "mu = np.tile(0.0,numDims)\n",
    "covmat = np.identity(numDims)\n",
    "numSamples = 10000\n",
    "\n",
    "X = np.random.multivariate_normal(mu,covmat,numSamples)\n",
    "print(\"matrix size =\", np.shape(X) )\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e49167abfdd974b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vig, ax = plt.subplots()\n",
    "ax.plot(X[:,0],X[:,1],'.')\n",
    "ax.set_xlabel('Dim 1')\n",
    "ax.set_ylabel('Dim 2')\n",
    "ax.set_title('ScatterPlot 2 dimensions')\n",
    "ax.axis('equal')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6fe1dd28abe9d6c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "empiricalMean = X.mean(0)\n",
    "empirical_covMat = np.cov(X.T)\n",
    "print(\"Empirical mean =\", empiricalMean)\n",
    "print(\"empirical covmat =\",empirical_covMat)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "661dc9ec712c7638"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "numBins = 40\n",
    "plt.hist2d(X[:,0],X[:,1],bins = numBins)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "my_dataframe = pd.DataFrame(X)\n",
    "my_dataframe.to_csv(\"normalData2.csv\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4635f6c797c4938f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import the necessary modules here\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd# For repeatability of results\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "# define the parameters of Gaussian\n",
    "numDims = 2\n",
    "mu = np.tile(0.0,numDims)\n",
    "rho = 0.7\n",
    "covmat = [[1,rho],[rho,1]]\n",
    "print(covmat)\n",
    "numSamples = 10000"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "932a2a3f656a4ebe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# generate samples from 2D normal distribution\n",
    "X = np.random.multivariate_normal(mu,covmat,numSamples)\n",
    "print(\"matrix size =\" , np.shape(X))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fed31906d65216a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# visualize the data\n",
    "plt.plot(X[:,0],X[:,1], '.')\n",
    "plt.axis('equal');\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdc5f80950234def"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute the mean and standard deviation of the generated samples\n",
    "empirical_Mean = X.mean(0)\n",
    "empirical_CovMat = np.cov(X.T)# X.T is the transpose of X as cov function requires data on columns\n",
    "print(\"empirical mean = \", empirical_Mean)\n",
    "print(\"empirical Covariance = \", empirical_CovMat)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb3601ef6287ebf7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot the histogram of the data\n",
    "numBins = 40\n",
    "plt.hist2d(X[:,0],X[:,1], bins=numBins)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#store the data in a csv file\n",
    "my_dataframe = pd.DataFrame(X) #converting numpy to dataframe\n",
    "my_dataframe.to_csv(\"normalData3.csv\")\n",
    "\n",
    "# end of task-3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6480f92288f2d741"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def euclidean_distance(p1, p2):\n",
    "    p1, p2 = np.array(p1), np.array(p2) #Ensure p1/p2 are NumPy Arrays\n",
    "    return np.sqrt(np.sum(np.square(p2-p1)))\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')\n",
    "avg_distances = []\n",
    "for n in range(2, 100):\n",
    "    avg_distances.append(np.mean([euclidean_distance(np.random.randint(low=-100, high=100, size=n), [0 for i in range(n)]) for p in range(500)]))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(2,100), avg_distances,'bs-')\n",
    "plt.plot( np.diff(avg_distances),'ro-')\n",
    "plt.xlabel('Number of dimensions')\n",
    "plt.ylabel('Euclidean Distance')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad9180c1ec8afade"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', delimiter=\",\", header=None).values\n",
    "print(data.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "266e4b41c9569692"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(data[:,0],data[:,1], '.', markersize=14)\n",
    "plt.axis('equal');\n",
    "plt.title('Original Data')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "342a3896abc37866"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now proceed with Implementing PCA using the following steps:\n",
    "\n",
    "1) normalise the data\n",
    "2) compute the covariance matrix of data\n",
    "3) compute the eigenvectors (U) and eigenvalues (S) of the covariance matrix"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f578738eca0b506"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mu = data.mean(axis=0) # mean of each col\n",
    "sigma = data.std(axis=0)  # std dev of each col\n",
    "\n",
    "Xnorm = (data - mu)/sigma\n",
    "print (Xnorm[0:5,:])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edf36f052a8fc9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculate the covariance matrix of normalised data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0ed3d9a509854be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Covariance matrix of normalized data\n",
    "m = len(Xnorm)\n",
    "covmat = np.dot(Xnorm.T, Xnorm)/m \n",
    "print(covmat)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73d0a3e5b245787c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculate the eigenvectors and eigenvalues of the covariance matrix"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cefae0105bd9475b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "S,U = np.linalg.eig(covmat)\n",
    "\n",
    "print('Eigen values: {}'.format(S))\n",
    "print('Eigen vectors:')\n",
    "print(U)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff90ebfd224bee1f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "So now we found out the principal components (\n",
    ") the set of axis that capture the maximum variation in data. What can we do this this now?\n",
    "\n",
    "We can do the following: 1. Decorrelation: Project our data onto \n",
    " to get decorrelated data 2. Dimensionality Reduction: Reduce \n",
    " to contain only those axis that contain maximum information. Project our data onto this reduced "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bc17e89f87ed1fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Z contains uncorrelated data  \n",
    "Z = np.dot(Xnorm,U)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7628a423c7955c2b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2 plots in one row\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(18,5))  # added size of each figs (width, height)\n",
    "fig.subplots_adjust(wspace=0.2) # leave some space between figs\n",
    "\n",
    "\n",
    "# plot for original data \n",
    "axs[0].scatter(data[:,0], data[:,1])\n",
    "axs[0].set_title(\"Original Data\")\n",
    "\n",
    "\n",
    "# plot for uncorrelated data after PCA\n",
    "axs[1].scatter(Z[:,0], Z[:,1])\n",
    "axs[1].set_title(\"Data after PCA\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba5abe1907018f26"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To reduce the dimensionality of our 2D data to 1D, we remove the principle component that captures the least variation. Our principle components, which are the eigen vectors of the covariance matrix are: U[:,0] and U[:,1]. By projecting our data Xnorm onto just U[:,0], we get a reduced Z in 1D.\n",
    "\n",
    "In general, we decide to keep \n",
    " eigenvectors in \n",
    " that captures maximum variation. Then our reduced data Znorm becomes: \n",
    "\n",
    "In this case, \n",
    ".k =1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abc176e796846990"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k = 1 # number of principal components to retain\n",
    "\n",
    "Ured =  U[:,0:k] # choose the first k principal components\n",
    "\n",
    "#project our data Xnorm onto Ured\n",
    "Zred = np.dot(Xnorm,Ured) \n",
    "\n",
    "print(Zred.shape)\n",
    "print(Ured.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89b179c6f02651f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "#recover our Xnorm data from Zred\n",
    "Xrec = np.dot(Zred, Ured.T)\n",
    "print(Xrec.shape)\n",
    "\n",
    "\n",
    "#Visualize the recovered data\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(18,5))  # added size of each figs (width, height)\n",
    "fig.subplots_adjust(wspace=0.2) # leave some space between figs\n",
    "\n",
    "\n",
    "# plot for Xnorm \n",
    "axs[0].scatter(Xnorm[:,0], Xnorm[:,1])\n",
    "axs[0].set_title(\"Normalised Original Data\")\n",
    "\n",
    "\n",
    "# plot for Xrec\n",
    "axs[1].scatter(Xrec[:,0], Xrec[:,1])\n",
    "axs[1].set_title(\"Recovered data after dimensionality reduction\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7124fc8a36baaf18"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To make this error term between  0 and 1, we divide it by the Frobenius norm of the original data Xnorm. Frobenius norm of a matrix is defined as the square root of the sum of the absolute squares of its elements.\n",
    "\n",
    "You can get a formal definition or watch a video illustrating a simple example if you need more information.\n",
    "\n",
    "In python, frobenius norm is implemented in linear algebra package of numpy. You can call it using linalg.norm(, 'fro')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46bbec9731739c37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rec_err = np.linalg.norm(Xnorm-Xrec, 'fro')/np.linalg.norm(Xnorm, 'fro')\n",
    "print(\"The reconstruction error is: {}\".format(rec_err))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8bfa372602c5d5f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PCA USING INBUILT FUNCTIONS "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed3fd3dacf547959"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv('train_wbcd.csv').dropna()\n",
    "data.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f10c572a348c631"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data normalization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c254cfd2c22681a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_norm=data.copy()\n",
    "mu = data_norm.iloc[:,2:].mean(axis=0) # mean of each col\n",
    "sigma = data_norm.iloc[:,2:].std(axis=0)  # std dev of each col\n",
    "data_norm.iloc[:,2:]=(data_norm.iloc[:,2:]-mu)/sigma"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99acf4a14063ddf6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Implement PCA"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8928e8d54b8e764"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#perform PCA using sklearn PCA implementation\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "Xnorm=data_norm.iloc[:,2:].copy().values\n",
    "pca.fit(Xnorm)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "feb07f0c98bc82bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#The amount of variance that each PC explains\n",
    "var= pca.explained_variance_ratio_\n",
    "print(var)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c3502011277099e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Cumulative Variance explains\n",
    "var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "print(var1)\n",
    "plt.plot(var1)\n",
    "plt.xlabel(\"Principal components\")\n",
    "plt.ylabel(\"Variance captured\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc72413974085ea0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1986614524d102eb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
