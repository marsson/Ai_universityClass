{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    " # 1) Create a MLP model with 10 hidden layers using  \"data.csv\" dataset and report performances using appropriate metrics."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43876 entries, 0 to 43875\n",
      "Columns: 101 entries, t_0 to malware\n",
      "dtypes: int64(101)\n",
      "memory usage: 33.8 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"data_10.csv\")\n",
    "dataset.info()\n",
    "dataset.isnull().values.any()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T23:18:12.937108Z",
     "start_time": "2023-09-27T23:18:12.421718Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "0    1\n1    1\n2    1\n3    1\n4    1\nName: malware, dtype: int64"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_dataframe = dataset.pop('malware')\n",
    "target_dataframe.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T23:18:13.903848Z",
     "start_time": "2023-09-27T23:18:13.895950Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "        t_0       t_1       t_2       t_3       t_4       t_5       t_6  \\\n0 -0.407815  1.040624  0.205229  0.381018  1.214753 -0.246378  0.806037   \n1 -0.804648 -0.057878  0.813612  0.278256 -0.217917 -0.870304  0.030828   \n2 -1.677680 -1.688988  1.925485 -1.057655  0.737197 -0.870304  1.256741   \n3 -0.804648 -0.057878  0.813612  0.278256 -0.217917 -0.870304  0.030828   \n4 -0.804648  0.474729 -0.654899  0.748027 -0.990435  1.001474 -0.960719   \n\n        t_7       t_8       t_9  ...      t_90      t_91      t_92      t_93  \\\n0  1.510980 -0.998545  0.760494  ... -0.521894 -1.065181  1.496827 -0.336061   \n1 -1.253474  0.439525 -0.942339  ... -1.237039 -0.937536  0.807549 -0.578494   \n2 -1.253474  1.458157 -0.942339  ... -0.446616 -1.141769 -0.740302 -0.497683   \n3 -1.253474  0.439525 -0.942339  ...  0.707653  0.683564  1.557290  0.647142   \n4  0.625133 -0.384369  0.086846  ... -1.487966  0.696328  1.049401 -1.615571   \n\n       t_94      t_95      t_96      t_97      t_98      t_99  \n0  0.090902  0.585610 -1.470810  0.634203 -1.396636 -1.072355  \n1 -1.332798  1.746557 -0.217968  0.160702  0.723982 -1.527663  \n2 -1.418220 -0.872652 -0.368309 -1.195815 -0.636415 -0.553809  \n3  1.955950  0.189190  0.696607  1.837152  0.897365  1.849207  \n4  0.631908  1.222715 -0.142797  1.299664 -0.262973  1.318014  \n\n[5 rows x 100 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>t_0</th>\n      <th>t_1</th>\n      <th>t_2</th>\n      <th>t_3</th>\n      <th>t_4</th>\n      <th>t_5</th>\n      <th>t_6</th>\n      <th>t_7</th>\n      <th>t_8</th>\n      <th>t_9</th>\n      <th>...</th>\n      <th>t_90</th>\n      <th>t_91</th>\n      <th>t_92</th>\n      <th>t_93</th>\n      <th>t_94</th>\n      <th>t_95</th>\n      <th>t_96</th>\n      <th>t_97</th>\n      <th>t_98</th>\n      <th>t_99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.407815</td>\n      <td>1.040624</td>\n      <td>0.205229</td>\n      <td>0.381018</td>\n      <td>1.214753</td>\n      <td>-0.246378</td>\n      <td>0.806037</td>\n      <td>1.510980</td>\n      <td>-0.998545</td>\n      <td>0.760494</td>\n      <td>...</td>\n      <td>-0.521894</td>\n      <td>-1.065181</td>\n      <td>1.496827</td>\n      <td>-0.336061</td>\n      <td>0.090902</td>\n      <td>0.585610</td>\n      <td>-1.470810</td>\n      <td>0.634203</td>\n      <td>-1.396636</td>\n      <td>-1.072355</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.804648</td>\n      <td>-0.057878</td>\n      <td>0.813612</td>\n      <td>0.278256</td>\n      <td>-0.217917</td>\n      <td>-0.870304</td>\n      <td>0.030828</td>\n      <td>-1.253474</td>\n      <td>0.439525</td>\n      <td>-0.942339</td>\n      <td>...</td>\n      <td>-1.237039</td>\n      <td>-0.937536</td>\n      <td>0.807549</td>\n      <td>-0.578494</td>\n      <td>-1.332798</td>\n      <td>1.746557</td>\n      <td>-0.217968</td>\n      <td>0.160702</td>\n      <td>0.723982</td>\n      <td>-1.527663</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.677680</td>\n      <td>-1.688988</td>\n      <td>1.925485</td>\n      <td>-1.057655</td>\n      <td>0.737197</td>\n      <td>-0.870304</td>\n      <td>1.256741</td>\n      <td>-1.253474</td>\n      <td>1.458157</td>\n      <td>-0.942339</td>\n      <td>...</td>\n      <td>-0.446616</td>\n      <td>-1.141769</td>\n      <td>-0.740302</td>\n      <td>-0.497683</td>\n      <td>-1.418220</td>\n      <td>-0.872652</td>\n      <td>-0.368309</td>\n      <td>-1.195815</td>\n      <td>-0.636415</td>\n      <td>-0.553809</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.804648</td>\n      <td>-0.057878</td>\n      <td>0.813612</td>\n      <td>0.278256</td>\n      <td>-0.217917</td>\n      <td>-0.870304</td>\n      <td>0.030828</td>\n      <td>-1.253474</td>\n      <td>0.439525</td>\n      <td>-0.942339</td>\n      <td>...</td>\n      <td>0.707653</td>\n      <td>0.683564</td>\n      <td>1.557290</td>\n      <td>0.647142</td>\n      <td>1.955950</td>\n      <td>0.189190</td>\n      <td>0.696607</td>\n      <td>1.837152</td>\n      <td>0.897365</td>\n      <td>1.849207</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.804648</td>\n      <td>0.474729</td>\n      <td>-0.654899</td>\n      <td>0.748027</td>\n      <td>-0.990435</td>\n      <td>1.001474</td>\n      <td>-0.960719</td>\n      <td>0.625133</td>\n      <td>-0.384369</td>\n      <td>0.086846</td>\n      <td>...</td>\n      <td>-1.487966</td>\n      <td>0.696328</td>\n      <td>1.049401</td>\n      <td>-1.615571</td>\n      <td>0.631908</td>\n      <td>1.222715</td>\n      <td>-0.142797</td>\n      <td>1.299664</td>\n      <td>-0.262973</td>\n      <td>1.318014</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 100 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "standardScaler = StandardScaler()\n",
    "standardScaler.fit(dataset)\n",
    "scaled_dataset = pd.DataFrame(standardScaler.transform(dataset), columns= dataset.columns.values)\n",
    "scaled_dataset.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T23:18:16.247999Z",
     "start_time": "2023-09-27T23:18:15.066065Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_dataset, target_dataframe, test_size=0.20, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T23:18:17.432595Z",
     "start_time": "2023-09-27T23:18:17.293076Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T23:18:22.529462Z",
     "start_time": "2023-09-27T23:18:19.551491Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-27 19:18:23.689227: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "878/878 [==============================] - 1s 984us/step - loss: 0.0819 - accuracy: 0.9792 - val_loss: 0.0506 - val_accuracy: 0.9855\n",
      "Epoch 2/10\n",
      "878/878 [==============================] - 1s 846us/step - loss: 0.0472 - accuracy: 0.9844 - val_loss: 0.0512 - val_accuracy: 0.9860\n",
      "Epoch 3/10\n",
      "878/878 [==============================] - 1s 855us/step - loss: 0.0361 - accuracy: 0.9877 - val_loss: 0.0508 - val_accuracy: 0.9858\n",
      "Epoch 4/10\n",
      "878/878 [==============================] - 1s 872us/step - loss: 0.0262 - accuracy: 0.9908 - val_loss: 0.0500 - val_accuracy: 0.9869\n",
      "Epoch 5/10\n",
      "878/878 [==============================] - 1s 878us/step - loss: 0.0209 - accuracy: 0.9932 - val_loss: 0.0504 - val_accuracy: 0.9853\n",
      "Epoch 6/10\n",
      "878/878 [==============================] - 1s 842us/step - loss: 0.0182 - accuracy: 0.9939 - val_loss: 0.0557 - val_accuracy: 0.9869\n",
      "Epoch 7/10\n",
      "878/878 [==============================] - 1s 832us/step - loss: 0.0188 - accuracy: 0.9944 - val_loss: 0.0610 - val_accuracy: 0.9842\n",
      "Epoch 8/10\n",
      "878/878 [==============================] - 1s 859us/step - loss: 0.0162 - accuracy: 0.9946 - val_loss: 0.0795 - val_accuracy: 0.9870\n",
      "Epoch 9/10\n",
      "878/878 [==============================] - 1s 827us/step - loss: 0.0132 - accuracy: 0.9957 - val_loss: 0.0888 - val_accuracy: 0.9870\n",
      "Epoch 10/10\n",
      "878/878 [==============================] - 1s 851us/step - loss: 0.0108 - accuracy: 0.9967 - val_loss: 0.1139 - val_accuracy: 0.9876\n",
      "275/275 [==============================] - 0s 373us/step - loss: 0.1235 - accuracy: 0.9839\n",
      "Loss: 0.12352403998374939\n",
      "Accuracy: 0.9839334487915039\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_dim=100))\n",
    "for _ in range(10):\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T23:18:32.064032Z",
     "start_time": "2023-09-27T23:18:23.504398Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " # 2) Analyse impact of different activation function with adam solver on the model.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "functions = [\"relu\", \"tanh\", \"sigmoid\", \"linear\", \"selu\", \"elu\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T23:18:41.871800Z",
     "start_time": "2023-09-27T23:18:41.861258Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "878/878 [==============================] - 1s 924us/step - loss: 0.0837 - accuracy: 0.9780 - val_loss: 0.0620 - val_accuracy: 0.9801\n",
      "Epoch 2/10\n",
      "878/878 [==============================] - 1s 840us/step - loss: 0.0490 - accuracy: 0.9853 - val_loss: 0.0535 - val_accuracy: 0.9853\n",
      "Epoch 3/10\n",
      "878/878 [==============================] - 1s 823us/step - loss: 0.0371 - accuracy: 0.9879 - val_loss: 0.0542 - val_accuracy: 0.9863\n",
      "Epoch 4/10\n",
      "878/878 [==============================] - 1s 839us/step - loss: 0.0279 - accuracy: 0.9905 - val_loss: 0.0527 - val_accuracy: 0.9860\n",
      "Epoch 5/10\n",
      "878/878 [==============================] - 1s 843us/step - loss: 0.0230 - accuracy: 0.9922 - val_loss: 0.0706 - val_accuracy: 0.9873\n",
      "Epoch 6/10\n",
      "878/878 [==============================] - 1s 865us/step - loss: 0.0184 - accuracy: 0.9935 - val_loss: 0.0851 - val_accuracy: 0.9869\n",
      "Epoch 7/10\n",
      "878/878 [==============================] - 1s 817us/step - loss: 0.0160 - accuracy: 0.9941 - val_loss: 0.0925 - val_accuracy: 0.9865\n",
      "Epoch 8/10\n",
      "878/878 [==============================] - 1s 818us/step - loss: 0.0145 - accuracy: 0.9959 - val_loss: 0.0771 - val_accuracy: 0.9812\n",
      "Epoch 9/10\n",
      "878/878 [==============================] - 1s 826us/step - loss: 0.0134 - accuracy: 0.9957 - val_loss: 0.0958 - val_accuracy: 0.9873\n",
      "Epoch 10/10\n",
      "878/878 [==============================] - 1s 814us/step - loss: 0.0115 - accuracy: 0.9965 - val_loss: 0.0938 - val_accuracy: 0.9887\n",
      "275/275 [==============================] - 0s 363us/step - loss: 0.1136 - accuracy: 0.9872\n",
      "For function relu we have:\n",
      "Loss: 0.11358131468296051\n",
      "Accuracy: 0.9872379302978516\n",
      "Epoch 1/10\n",
      "878/878 [==============================] - 1s 902us/step - loss: 0.0917 - accuracy: 0.9766 - val_loss: 0.0569 - val_accuracy: 0.9853\n",
      "Epoch 2/10\n",
      "878/878 [==============================] - 1s 841us/step - loss: 0.0501 - accuracy: 0.9849 - val_loss: 0.0470 - val_accuracy: 0.9883\n",
      "Epoch 3/10\n",
      "878/878 [==============================] - 1s 855us/step - loss: 0.0391 - accuracy: 0.9875 - val_loss: 0.0439 - val_accuracy: 0.9876\n",
      "Epoch 4/10\n",
      "878/878 [==============================] - 1s 815us/step - loss: 0.0293 - accuracy: 0.9902 - val_loss: 0.0532 - val_accuracy: 0.9860\n",
      "Epoch 5/10\n",
      "878/878 [==============================] - 1s 846us/step - loss: 0.0232 - accuracy: 0.9920 - val_loss: 0.0539 - val_accuracy: 0.9866\n",
      "Epoch 6/10\n",
      "878/878 [==============================] - 1s 833us/step - loss: 0.0179 - accuracy: 0.9940 - val_loss: 0.0553 - val_accuracy: 0.9873\n",
      "Epoch 7/10\n",
      "878/878 [==============================] - 1s 816us/step - loss: 0.0154 - accuracy: 0.9948 - val_loss: 0.0725 - val_accuracy: 0.9865\n",
      "Epoch 8/10\n",
      "878/878 [==============================] - 1s 813us/step - loss: 0.0139 - accuracy: 0.9959 - val_loss: 0.0657 - val_accuracy: 0.9879\n",
      "Epoch 9/10\n",
      "878/878 [==============================] - 1s 832us/step - loss: 0.0154 - accuracy: 0.9956 - val_loss: 0.0678 - val_accuracy: 0.9845\n",
      "Epoch 10/10\n",
      "878/878 [==============================] - 1s 831us/step - loss: 0.0110 - accuracy: 0.9964 - val_loss: 0.0778 - val_accuracy: 0.9883\n",
      "275/275 [==============================] - 0s 366us/step - loss: 0.1033 - accuracy: 0.9861\n",
      "For function tanh we have:\n",
      "Loss: 0.10327538847923279\n",
      "Accuracy: 0.9860984683036804\n",
      "Epoch 1/10\n",
      "878/878 [==============================] - 1s 920us/step - loss: 0.0841 - accuracy: 0.9788 - val_loss: 0.0538 - val_accuracy: 0.9856\n",
      "Epoch 2/10\n",
      "878/878 [==============================] - 1s 875us/step - loss: 0.0497 - accuracy: 0.9845 - val_loss: 0.0497 - val_accuracy: 0.9860\n",
      "Epoch 3/10\n",
      "878/878 [==============================] - 1s 859us/step - loss: 0.0369 - accuracy: 0.9875 - val_loss: 0.0547 - val_accuracy: 0.9846\n",
      "Epoch 4/10\n",
      "878/878 [==============================] - 1s 826us/step - loss: 0.0276 - accuracy: 0.9904 - val_loss: 0.0654 - val_accuracy: 0.9858\n",
      "Epoch 5/10\n",
      "878/878 [==============================] - 1s 813us/step - loss: 0.0210 - accuracy: 0.9929 - val_loss: 0.0678 - val_accuracy: 0.9843\n",
      "Epoch 6/10\n",
      "878/878 [==============================] - 1s 816us/step - loss: 0.0187 - accuracy: 0.9937 - val_loss: 0.0754 - val_accuracy: 0.9830\n",
      "Epoch 7/10\n",
      "878/878 [==============================] - 1s 844us/step - loss: 0.0157 - accuracy: 0.9949 - val_loss: 0.0761 - val_accuracy: 0.9858\n",
      "Epoch 8/10\n",
      "878/878 [==============================] - 1s 858us/step - loss: 0.0139 - accuracy: 0.9959 - val_loss: 0.1008 - val_accuracy: 0.9849\n",
      "Epoch 9/10\n",
      "878/878 [==============================] - 1s 992us/step - loss: 0.0144 - accuracy: 0.9958 - val_loss: 0.0884 - val_accuracy: 0.9869\n",
      "Epoch 10/10\n",
      "878/878 [==============================] - 1s 855us/step - loss: 0.0113 - accuracy: 0.9966 - val_loss: 0.1228 - val_accuracy: 0.9866\n",
      "275/275 [==============================] - 0s 370us/step - loss: 0.1609 - accuracy: 0.9860\n",
      "For function sigmoid we have:\n",
      "Loss: 0.16085006296634674\n",
      "Accuracy: 0.9859845042228699\n",
      "Epoch 1/10\n",
      "878/878 [==============================] - 1s 897us/step - loss: 0.0808 - accuracy: 0.9798 - val_loss: 0.0622 - val_accuracy: 0.9852\n",
      "Epoch 2/10\n",
      "878/878 [==============================] - 1s 813us/step - loss: 0.0516 - accuracy: 0.9840 - val_loss: 0.0466 - val_accuracy: 0.9870\n",
      "Epoch 3/10\n",
      "878/878 [==============================] - 1s 818us/step - loss: 0.0397 - accuracy: 0.9873 - val_loss: 0.0519 - val_accuracy: 0.9866\n",
      "Epoch 4/10\n",
      "878/878 [==============================] - 1s 821us/step - loss: 0.0324 - accuracy: 0.9875 - val_loss: 0.0448 - val_accuracy: 0.9879\n",
      "Epoch 5/10\n",
      "878/878 [==============================] - 1s 820us/step - loss: 0.0260 - accuracy: 0.9911 - val_loss: 0.0563 - val_accuracy: 0.9893\n",
      "Epoch 6/10\n",
      "878/878 [==============================] - 1s 815us/step - loss: 0.0204 - accuracy: 0.9933 - val_loss: 0.0669 - val_accuracy: 0.9879\n",
      "Epoch 7/10\n",
      "878/878 [==============================] - 1s 817us/step - loss: 0.0155 - accuracy: 0.9948 - val_loss: 0.0939 - val_accuracy: 0.9886\n",
      "Epoch 8/10\n",
      "878/878 [==============================] - 1s 827us/step - loss: 0.0171 - accuracy: 0.9948 - val_loss: 0.0551 - val_accuracy: 0.9868\n",
      "Epoch 9/10\n",
      "878/878 [==============================] - 1s 813us/step - loss: 0.0142 - accuracy: 0.9952 - val_loss: 0.0855 - val_accuracy: 0.9883\n",
      "Epoch 10/10\n",
      "878/878 [==============================] - 1s 817us/step - loss: 0.0121 - accuracy: 0.9959 - val_loss: 0.0530 - val_accuracy: 0.9869\n",
      "275/275 [==============================] - 0s 365us/step - loss: 0.0712 - accuracy: 0.9848\n",
      "For function linear we have:\n",
      "Loss: 0.07119766622781754\n",
      "Accuracy: 0.9848450422286987\n",
      "Epoch 1/10\n",
      "878/878 [==============================] - 1s 888us/step - loss: 0.0799 - accuracy: 0.9801 - val_loss: 0.0567 - val_accuracy: 0.9863\n",
      "Epoch 2/10\n",
      "878/878 [==============================] - 1s 817us/step - loss: 0.0490 - accuracy: 0.9856 - val_loss: 0.0490 - val_accuracy: 0.9849\n",
      "Epoch 3/10\n",
      "878/878 [==============================] - 1s 819us/step - loss: 0.0359 - accuracy: 0.9884 - val_loss: 0.0475 - val_accuracy: 0.9880\n",
      "Epoch 4/10\n",
      "878/878 [==============================] - 1s 830us/step - loss: 0.0291 - accuracy: 0.9908 - val_loss: 0.0521 - val_accuracy: 0.9882\n",
      "Epoch 5/10\n",
      "878/878 [==============================] - 1s 830us/step - loss: 0.0206 - accuracy: 0.9929 - val_loss: 0.0769 - val_accuracy: 0.9860\n",
      "Epoch 6/10\n",
      "878/878 [==============================] - 1s 820us/step - loss: 0.0191 - accuracy: 0.9933 - val_loss: 0.0996 - val_accuracy: 0.9869\n",
      "Epoch 7/10\n",
      "878/878 [==============================] - 1s 846us/step - loss: 0.0168 - accuracy: 0.9942 - val_loss: 0.0954 - val_accuracy: 0.9889\n",
      "Epoch 8/10\n",
      "878/878 [==============================] - 1s 835us/step - loss: 0.0140 - accuracy: 0.9952 - val_loss: 0.0701 - val_accuracy: 0.9876\n",
      "Epoch 9/10\n",
      "878/878 [==============================] - 1s 817us/step - loss: 0.0141 - accuracy: 0.9951 - val_loss: 0.0583 - val_accuracy: 0.9889\n",
      "Epoch 10/10\n",
      "878/878 [==============================] - 1s 825us/step - loss: 0.0125 - accuracy: 0.9958 - val_loss: 0.0853 - val_accuracy: 0.9883\n",
      "275/275 [==============================] - 0s 362us/step - loss: 0.1037 - accuracy: 0.9859\n",
      "For function selu we have:\n",
      "Loss: 0.10374459624290466\n",
      "Accuracy: 0.9858705401420593\n",
      "Epoch 1/10\n",
      "878/878 [==============================] - 1s 883us/step - loss: 0.0832 - accuracy: 0.9788 - val_loss: 0.0586 - val_accuracy: 0.9843\n",
      "Epoch 2/10\n",
      "878/878 [==============================] - 1s 817us/step - loss: 0.0494 - accuracy: 0.9845 - val_loss: 0.0507 - val_accuracy: 0.9858\n",
      "Epoch 3/10\n",
      "878/878 [==============================] - 1s 821us/step - loss: 0.0369 - accuracy: 0.9882 - val_loss: 0.0461 - val_accuracy: 0.9866\n",
      "Epoch 4/10\n",
      "878/878 [==============================] - 1s 827us/step - loss: 0.0265 - accuracy: 0.9910 - val_loss: 0.0483 - val_accuracy: 0.9870\n",
      "Epoch 5/10\n",
      "878/878 [==============================] - 1s 819us/step - loss: 0.0213 - accuracy: 0.9927 - val_loss: 0.0781 - val_accuracy: 0.9873\n",
      "Epoch 6/10\n",
      "878/878 [==============================] - 1s 827us/step - loss: 0.0174 - accuracy: 0.9936 - val_loss: 0.0575 - val_accuracy: 0.9842\n",
      "Epoch 7/10\n",
      "878/878 [==============================] - 1s 937us/step - loss: 0.0152 - accuracy: 0.9947 - val_loss: 0.0606 - val_accuracy: 0.9877\n",
      "Epoch 8/10\n",
      "878/878 [==============================] - 1s 824us/step - loss: 0.0122 - accuracy: 0.9954 - val_loss: 0.1209 - val_accuracy: 0.9872\n",
      "Epoch 9/10\n",
      "878/878 [==============================] - 1s 839us/step - loss: 0.0146 - accuracy: 0.9955 - val_loss: 0.0821 - val_accuracy: 0.9838\n",
      "Epoch 10/10\n",
      "878/878 [==============================] - 1s 835us/step - loss: 0.0088 - accuracy: 0.9969 - val_loss: 0.1460 - val_accuracy: 0.9873\n",
      "275/275 [==============================] - 0s 361us/step - loss: 0.1889 - accuracy: 0.9861\n",
      "For function elu we have:\n",
      "Loss: 0.18890415132045746\n",
      "Accuracy: 0.9860984683036804\n"
     ]
    }
   ],
   "source": [
    "for function in functions:\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_dim=100))\n",
    "    for _ in range(10):\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"For function {function} we have:\")\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Accuracy: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T23:19:32.285942Z",
     "start_time": "2023-09-27T23:18:43.466294Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The comparison between the different activation functions was:\n",
    "ReLU Activation Function\n",
    "Test Loss: 0.1136\n",
    "Test Accuracy: 98.72%\n",
    "\n",
    "Tanh Activation Function\n",
    "Test Loss: 0.1033\n",
    "Test Accuracy: 98.61%\n",
    "\n",
    "Sigmoid Activation Function\n",
    "Test Loss: 0.1609\n",
    "Test Accuracy: 98.60%\n",
    "\n",
    "Linear Activation Function\n",
    "Test Loss: 0.0712\n",
    "Test Accuracy: 98.48%\n",
    "\n",
    "SELU Activation Function\n",
    "Test Loss: 0.1037\n",
    "Test Accuracy: 98.59%\n",
    "\n",
    "ELU Activation Function\n",
    "Test Loss: 0.1889\n",
    "Test Accuracy: 98.61%"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3) Explain your findings and report the best performance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From all the activations we tried the best result was achieved by ReLU activation function. It had a 98.6% precision and 0.1136 test loss. ReLU is generally good for hidden layers in deep neural networks as to it mitigates the vanishing gradient problem.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
